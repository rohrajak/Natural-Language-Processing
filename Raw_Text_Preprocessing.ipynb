{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <b>Overview</b>\n",
        "\n",
        "This example demonstrates the fundamental steps of raw text preprocessing in natural language processing (NLP). The code first imports essential Python libraries, then uploads a text dataset for analysis. After loading the data, it tokenizes the sentences—breaking the text into individual words or phrases—preparing it for further linguistic processing.\n",
        "\n"
      ],
      "metadata": {
        "id": "QVFY1t_I-bBt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smP2GrulNXQ-",
        "outputId": "869619f9-d1f9-48eb-8a46-9f4c4bac69a0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.11/dist-packages (2.6.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install autocorrect # Install the autocorrect package\n",
        "\n",
        "import nltk # Import the nltk package\n",
        "nltk.download('punkt') # Download the Punkt tokenizer\n",
        "nltk.download('punkt_tab') # Download the Punkt tokenizer\n",
        "from nltk.tokenize import word_tokenize # Import the word_tokenize function# Import the word_tokenize function\n",
        "nltk.download('averaged_perceptron_tagger') # Download the POS tagger\n",
        "nltk.download('stopwords') # Download the stopwords\n",
        "nltk.download('wordnet') # Download the WordNet lemmatizer\n",
        "from nltk import word_tokenize   # Import the word_tokenize function\n",
        "from nltk.stem.wordnet import WordNetLemmatizer   # Import the WordNet lemmatizer\n",
        "from nltk.corpus import stopwords   # Import the stopwords\n",
        "from autocorrect import spell   # Import the spell checker\n",
        "from nltk.wsd import lesk   # Import the Lesk algorithm\n",
        "from nltk.tokenize import sent_tokenize  # type: ignore # Import the sentence tokenizer\n",
        "import string   # Import the string module"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <h2>Libraries Info:</h2>\n",
        "\n",
        "  <ol type=\"1\">\n",
        "    <li>autocorrect: This package provides automatic spelling correction for words. It is useful in text preprocessing to correct common spelling mistakes.</li>\n",
        "    <li>nltk (Natural Language Toolkit): A popular Python library for NLP tasks such as tokenization, lemmatization, part-of-speech tagging, and more.</li>\n",
        "    <li>nltk.download('punkt'): Downloads the Punkt tokenizer, which is a pre-trained sentence and word tokenizer used for splitting text into sentences and words.</li>\n",
        "    <li>word_tokenize (from nltk.tokenize): A function that tokenizes (splits) a given text into words.</li>\n",
        "    <li>nltk.download('averaged_perceptron_tagger'): Downloads the averaged perceptron tagger, which is used for part-of-speech (POS) tagging.</li>\n",
        "    <li>nltk.download('stopwords'): Downloads a predefined list of stopwords (common words like \"the,\" \"is,\" etc.) that are often removed in text processing.</li>\n",
        "    <li>nltk.download('wordnet'): Downloads the WordNet lexical database, which is used for word sense disambiguation and lemmatization.</li>\n",
        "    <li>word_tokenize (from nltk): Re-imports the word_tokenize function for tokenizing text into words. (This is redundant in your code.)</li>\n",
        "    <li>WordNetLemmatizer (from nltk.stem.wordnet): A lemmatizer that reduces words to their base or root form using the WordNet database.</li>\n",
        "    <li>stopwords (from nltk.corpus): Provides a predefined list of common stopwords that can be filtered out from text during preprocessing.</li>\n",
        "    <li>spell (from autocorrect): Provides a spelling correction function that automatically corrects misspelled words. (Note: The correct import should be from autocorrect import Speller and then Speller() instead of spell.)</li>\n",
        "    <li>lesk (from nltk.wsd): Implements the Lesk algorithm, a word sense disambiguation technique that determines the correct meaning of a word based on its surrounding context.</li>\n",
        "    <li>sent_tokenize (from nltk.tokenize): A function that splits text into individual sentences.</li>\n",
        "    <li>string: A built-in Python module that provides tools for working with textual data, including string manipulation and punctuation removal.</li>\n",
        "  </ol>"
      ],
      "metadata": {
        "id": "7Z1TuupHDEsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = open(\"/content/file.txt\", \"r\").read()   # Read the text file"
      ],
      "metadata": {
        "id": "23HdCrmrNwTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IE2XXx37s6z",
        "outputId": "209b10ef-fb7b-4fc3-ad23-d0d70e7a9872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In this book authored by Sohom Ghosh and Dwight Gunning, we shall learnning how to pracess Natueral Language and extract insights from it. The first four chapter will introduce you to the basics of NLP. Later chapters will describe how to deal with complex NLP prajects. If you want to get early access of it, you should book your order now.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(sentence)   # Tokenize the words"
      ],
      "metadata": {
        "id": "hB_B_QcWNzf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words[0:10])   # Print the first 20 words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2Uoi_2UN2dI",
        "outputId": "945bad99-272d-46fc-d934-2bba56ad1509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'this', 'book', 'authored', 'by', 'Sohom', 'Ghosh', 'and', 'Dwight', 'Gunning']\n"
          ]
        }
      ]
    }
  ]
}